# === Step 5: Generative QA ì›¹ ì„œë¹„ìŠ¤ ===
# ìˆ˜ê°•ìƒ ê³¼ì œ: TODO ë¶€ë¶„ì„ ì™„ì„±í•˜ì—¬ Flask ê¸°ë°˜ ìƒì„±í˜• QA ì›¹ ì„œë¹„ìŠ¤ë¥¼ êµ¬í˜„í•˜ì„¸ìš”.
# Extractive QAì™€ì˜ ì°¨ì´ì : í…ìŠ¤íŠ¸ ìƒì„± ê¸°ë°˜ ì‘ë‹µê³¼ ì°½ì˜ì  ë‹µë³€ ì§€ì›!

import logging
import os
from pathlib import Path
from typing import Dict, Any

import torch
import torch.nn.functional as F
import typer
from flask import Flask, request, jsonify, render_template
from flask_classful import FlaskView, route

from transformers import AutoTokenizer, AutoModelForSeq2SeqLM

logger = logging.getLogger(__name__)

class GenerativeQAModel:
    """ìƒì„±í˜• ì§ˆì˜ì‘ë‹µ ëª¨ë¸ í´ë˜ìŠ¤"""
    
    def __init__(self, pretrained: str, server_page: str, num_beams: int = 5, max_length: int = 50):
        """
        Generative QA ëª¨ë¸ ì´ˆê¸°í™”
        
        Args:
            pretrained: ì‚¬ì „í•™ìŠµëœ T5 QA ëª¨ë¸ ê²½ë¡œ ë˜ëŠ” Hugging Face Hub ID
            server_page: ì›¹ í…œí”Œë¦¿ íŒŒì¼ëª…
            num_beams: Beam Search í­
            max_length: ìµœëŒ€ ìƒì„± ê¸¸ì´
        """
        self.server_page = server_page
        self.num_beams = num_beams
        self.max_length = max_length

        # TODO: T5 ëª¨ë¸ ë¡œë“œ
        logger.info(f"Loading T5 model from {pretrained}")
        # íŒíŠ¸: AutoTokenizer, AutoModelForSeq2SeqLM ì‚¬ìš©
        self.tokenizer = # TODO: ì™„ì„±í•˜ì„¸ìš”
        self.model = # TODO: ì™„ì„±í•˜ì„¸ìš”
        
        # TODO: ëª¨ë¸ì„ í‰ê°€ ëª¨ë“œë¡œ ì„¤ì •
        # TODO: ì™„ì„±í•˜ì„¸ìš”

    def run_server(self, server: Flask, *args, **kwargs):
        """Flask ì›¹ ì„œë²„ ì‹¤í–‰"""
        # TODO: WebAPI í´ë˜ìŠ¤ë¥¼ Flask ì•±ì— ë“±ë¡
        # íŒíŠ¸: GenerativeQAModel.WebAPI.register() ì‚¬ìš©
        # TODO: ì™„ì„±í•˜ì„¸ìš”
        
        # TODO: ì„œë²„ ì‹¤í–‰
        # TODO: ì™„ì„±í•˜ì„¸ìš”

    def infer_one(self, question: str, context: str) -> Dict[str, Any]:
        """
        ë‹¨ì¼ ì§ˆë¬¸-ì»¨í…ìŠ¤íŠ¸ ìŒì— ëŒ€í•œ ìƒì„±í˜• ë‹µë³€
        
        Args:
            question: ì§ˆë¬¸ ë¬¸ìì—´
            context: ì»¨í…ìŠ¤íŠ¸ ë¬¸ìì—´
            
        Returns:
            dict: ìƒì„±ëœ ë‹µë³€ê³¼ ê´€ë ¨ ì •ë³´ë¥¼ í¬í•¨í•œ ë”•ì…”ë„ˆë¦¬
        """
        # TODO: ì…ë ¥ ìœ íš¨ì„± ê²€ì‚¬
        if not question.strip():
            return {"question": question, "context": context, "answer": "(ì§ˆë¬¸ì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.)"}
        if not context.strip():
            return {"question": question, "context": context, "answer": "(ì»¨í…ìŠ¤íŠ¸ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.)"}

        # TODO: T5 ì…ë ¥ í˜•ì‹ êµ¬ì„±
        # íŒíŠ¸: "question: {ì§ˆë¬¸} context: {ì»¨í…ìŠ¤íŠ¸}" í˜•íƒœ
        input_text = # TODO: ì™„ì„±í•˜ì„¸ìš”
        
        # TODO: ì…ë ¥ í† í¬ë‚˜ì´ì œì´ì…˜
        # íŒíŠ¸: self.tokenizer() ì‚¬ìš©, return_tensors="pt", truncation=True, padding=True
        inputs = # TODO: ì™„ì„±í•˜ì„¸ìš”
        
        # TODO: í…ìŠ¤íŠ¸ ìƒì„± (gradient ê³„ì‚° ì—†ì´)
        with torch.no_grad():
            # TODO: ì ìˆ˜ì™€ í•¨ê»˜ ìƒì„±
            # íŒíŠ¸: self.model.generate() ì‚¬ìš©, return_dict_in_generate=True, output_scores=True
            outputs = self.model.generate(
                # TODO: í•„ìš”í•œ ì¸ìˆ˜ë“¤ì„ ì™„ì„±í•˜ì„¸ìš”
                # input_ids, attention_mask, max_length, num_beams,
                # return_dict_in_generate, output_scores
            )

        # TODO: ìƒì„±ëœ ë‹µë³€ ë””ì½”ë”©
        # íŒíŠ¸: self.tokenizer.decode() ì‚¬ìš©, skip_special_tokens=True
        answer = # TODO: ì™„ì„±í•˜ì„¸ìš”

        # TODO: ì‹ ë¢°ë„ ì ìˆ˜ ê³„ì‚° (í† í° í™•ë¥  ê¸°ë°˜)
        if hasattr(outputs, 'scores') and outputs.scores:
            token_probs = []
            for i, token_id in enumerate(outputs.sequences[0]):
                if i == 0:  # ì‹œì‘ í† í° ì œì™¸
                    continue
                if i-1 < len(outputs.scores):
                    # TODO: ì†Œí”„íŠ¸ë§¥ìŠ¤ë¡œ í™•ë¥  ë³€í™˜
                    token_prob = # TODO: F.softmax(outputs.scores[i-1], dim=-1)[0, token_id].item()
                    token_probs.append(token_prob)
            
            # TODO: ì „ì²´ ì ìˆ˜ ê³„ì‚° (í† í° í™•ë¥ ë“¤ì˜ ê³±)
            if token_probs:
                score = # TODO: torch.prod(torch.tensor(token_probs)).item()
            else:
                score = 0.0
        else:
            score = 0.0

        return {
            "question": question,
            "context": context,
            "answer": answer,
            "score": round(score, 4),
            "model_type": "generative",
            "generation_params": {
                "num_beams": self.num_beams,
                "max_length": self.max_length
            }
        }

    def infer_creative(self, question: str, context: str, creative_params: Dict[str, Any] = None) -> Dict[str, Any]:
        """
        ì°½ì˜ì  ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ ìƒì„± (Generative QAì˜ ì¥ì !)
        
        Args:
            question: ì§ˆë¬¸ ë¬¸ìì—´
            context: ì»¨í…ìŠ¤íŠ¸ ë¬¸ìì—´
            creative_params: ì°½ì˜ì  ìƒì„±ì„ ìœ„í•œ íŒŒë¼ë¯¸í„°ë“¤
            
        Returns:
            dict: ì°½ì˜ì  ë‹µë³€ê³¼ ê´€ë ¨ ì •ë³´
        """
        if creative_params is None:
            creative_params = {}
        
        # TODO: ì°½ì˜ì  ìƒì„±ì„ ìœ„í•œ íŒŒë¼ë¯¸í„° ì„¤ì •
        creative_num_beams = creative_params.get("num_beams", 3)
        creative_max_length = creative_params.get("max_length", 100)
        do_sample = creative_params.get("do_sample", True)
        temperature = creative_params.get("temperature", 0.8)
        top_p = creative_params.get("top_p", 0.9)

        # TODO: T5 ì…ë ¥ í˜•ì‹ êµ¬ì„±
        input_text = # TODO: ì™„ì„±í•˜ì„¸ìš”
        inputs = # TODO: ì™„ì„±í•˜ì„¸ìš”
        
        with torch.no_grad():
            # TODO: ì°½ì˜ì  íŒŒë¼ë¯¸í„°ë¡œ ìƒì„±
            outputs = self.model.generate(
                input_ids=inputs["input_ids"],
                attention_mask=inputs["attention_mask"],
                # TODO: ì°½ì˜ì  ìƒì„± íŒŒë¼ë¯¸í„°ë“¤ì„ ì™„ì„±í•˜ì„¸ìš”
                # max_length, num_beams, do_sample, temperature, top_p
            )

        # TODO: ë‹µë³€ ë””ì½”ë”©
        answer = # TODO: ì™„ì„±í•˜ì„¸ìš”

        return {
            "question": question,
            "context": context,
            "answer": answer,
            "model_type": "generative_creative",
            "generation_params": {
                "num_beams": creative_num_beams,
                "max_length": creative_max_length,
                "do_sample": do_sample,
                "temperature": temperature,
                "top_p": top_p
            }
        }

    class WebAPI(FlaskView):
        """Flask ê¸°ë°˜ ì›¹ API í´ë˜ìŠ¤"""
        
        def __init__(self, model: "GenerativeQAModel"):
            """
            WebAPI ì´ˆê¸°í™”
            
            Args:
                model: GenerativeQAModel ì¸ìŠ¤í„´ìŠ¤
            """
            self.model = model

        @route('/')
        def index(self):
            """ë©”ì¸ í˜ì´ì§€ ë Œë”ë§"""
            # TODO: HTML í…œí”Œë¦¿ ë Œë”ë§
            # íŒíŠ¸: render_template() ì‚¬ìš©, self.model.server_page
            return # TODO: ì™„ì„±í•˜ì„¸ìš”

        @route('/api', methods=['POST'])
        def api(self):
            """
            í‘œì¤€ ìƒì„±í˜• QA API ì—”ë“œí¬ì¸íŠ¸
            
            POST /api
            Request Body: {"question": "ì§ˆë¬¸", "context": "ì»¨í…ìŠ¤íŠ¸"}
            
            Returns:
                JSON: ìƒì„±ëœ ë‹µë³€ê³¼ ê´€ë ¨ ì •ë³´
            """
            # TODO: JSON ìš”ì²­ ë°ì´í„° íŒŒì‹±
            data = # TODO: request.json
            question = # TODO: dataì—ì„œ "question" ì¶”ì¶œ, ê¸°ë³¸ê°’ ""
            context = # TODO: dataì—ì„œ "context" ì¶”ì¶œ, ê¸°ë³¸ê°’ ""
            
            # TODO: ìƒì„±í˜• QA ëª¨ë¸ë¡œ ë‹µë³€ ìƒì„±
            result = # TODO: self.model.infer_one() í˜¸ì¶œ
            
            # TODO: JSON í˜•íƒœë¡œ ê²°ê³¼ ë°˜í™˜
            return # TODO: jsonify() ì‚¬ìš©

        @route('/api/creative', methods=['POST'])
        def api_creative(self):
            """
            ì°½ì˜ì  ìƒì„±í˜• QA API ì—”ë“œí¬ì¸íŠ¸ (Generative QAì˜ ê³ ìœ  ê¸°ëŠ¥!)
            
            POST /api/creative
            Request Body: {
                "question": "ì§ˆë¬¸", 
                "context": "ì»¨í…ìŠ¤íŠ¸",
                "creative_params": {...}
            }
            
            Returns:
                JSON: ì°½ì˜ì  ë‹µë³€ê³¼ ìƒì„± íŒŒë¼ë¯¸í„°
            """
            # TODO: ì°½ì˜ì  ìš”ì²­ ë°ì´í„° íŒŒì‹±
            data = request.json
            question = data.get("question", "")
            context = data.get("context", "")
            creative_params = data.get("creative_params", {})
            
            # TODO: ì°½ì˜ì  ë‹µë³€ ìƒì„±
            result = # TODO: self.model.infer_creative() í˜¸ì¶œ
            
            return jsonify(result)

        @route('/api/compare', methods=['POST'])
        def api_compare(self):
            """
            í‘œì¤€ vs ì°½ì˜ì  ìƒì„± ë¹„êµ API
            
            POST /api/compare  
            Request Body: {"question": "ì§ˆë¬¸", "context": "ì»¨í…ìŠ¤íŠ¸"}
            
            Returns:
                JSON: ë‘ ê°€ì§€ ë°©ì‹ì˜ ë‹µë³€ ë¹„êµ
            """
            # TODO: ë¹„êµ ìš”ì²­ ë°ì´í„° íŒŒì‹±
            data = request.json
            question = data.get("question", "")
            context = data.get("context", "")
            
            # TODO: í‘œì¤€ ë‹µë³€ ìƒì„±
            standard_result = # TODO: self.model.infer_one() í˜¸ì¶œ
            
            # TODO: ì°½ì˜ì  ë‹µë³€ ìƒì„±
            creative_result = # TODO: self.model.infer_creative() í˜¸ì¶œ
            
            return jsonify({
                "question": question,
                "context": context,
                "standard_answer": standard_result,
                "creative_answer": creative_result,
                "comparison": {
                    "standard_length": len(standard_result["answer"].split()),
                    "creative_length": len(creative_result["answer"].split()),
                    "length_difference": len(creative_result["answer"].split()) - len(standard_result["answer"].split())
                }
            })

        @route('/health')
        def health(self):
            """ì„œë¹„ìŠ¤ ìƒíƒœ í™•ì¸ ì—”ë“œí¬ì¸íŠ¸"""
            # TODO: ì„œë¹„ìŠ¤ ìƒíƒœ ì •ë³´ ë°˜í™˜
            return jsonify({
                "status": "healthy",
                "model_type": "generative_qa",
                "model_loaded": self.model.model is not None,
                "tokenizer_loaded": self.model.tokenizer is not None,
                "generation_params": {
                    "num_beams": self.model.num_beams,
                    "max_length": self.model.max_length
                }
            })


# === CLI ì• í”Œë¦¬ì¼€ì´ì…˜ ===

main = typer.Typer()

@main.command()
def serve(
    # TODO: CLI ì˜µì…˜ë“¤ ì •ì˜
    pretrained: str = typer.Option(
        # TODO: ê¸°ë³¸ê°’ ì„¤ì • (ë¡œì»¬ ì²´í¬í¬ì¸íŠ¸ ë˜ëŠ” Hub ID)
        help="ì‚¬ì „í•™ìŠµëœ T5 QA ëª¨ë¸ ê²½ë¡œ ë˜ëŠ” Hugging Face Hub ID"
    ),
    server_host: str = typer.Option(
        # TODO: ê¸°ë³¸ê°’ "0.0.0.0",
        help="ì„œë²„ í˜¸ìŠ¤íŠ¸ ì£¼ì†Œ"
    ),
    server_port: int = typer.Option(
        # TODO: ê¸°ë³¸ê°’ 9165,
        help="ì„œë²„ í¬íŠ¸ ë²ˆí˜¸"
    ),
    server_page: str = typer.Option(
        # TODO: ê¸°ë³¸ê°’ "serve_qa_seq2seq.html",
        help="ì›¹ í…œí”Œë¦¿ íŒŒì¼ëª…"
    ),
    num_beams: int = typer.Option(
        # TODO: ê¸°ë³¸ê°’ 5,
        help="Beam Search í­"
    ),
    max_length: int = typer.Option(
        # TODO: ê¸°ë³¸ê°’ 50,
        help="ìµœëŒ€ ìƒì„± ê¸¸ì´"
    ),
    debug: bool = typer.Option(
        # TODO: ê¸°ë³¸ê°’ False,
        help="Flask ë””ë²„ê·¸ ëª¨ë“œ í™œì„±í™”"
    ),
):
    """
    ìƒì„±í˜• QA ì›¹ ì„œë¹„ìŠ¤ ì‹¤í–‰
    
    ê¸°ëŠ¥:
    - ì‹¤ì‹œê°„ ìƒì„±í˜• ì§ˆì˜ì‘ë‹µ ì›¹ ì¸í„°í˜ì´ìŠ¤ ì œê³µ
    - í‘œì¤€ ë° ì°½ì˜ì  ìƒì„± API ì—”ë“œí¬ì¸íŠ¸ ì œê³µ
    - ìƒì„± íŒŒë¼ë¯¸í„° ì¡°ì • ì§€ì›
    - Extractive vs Generative ë¹„êµ ê°€ëŠ¥
    """
    # TODO: ë¡œê¹… ì„¤ì •
    logging.basicConfig(level=logging.INFO)
    os.environ["TOKENIZERS_PARALLELISM"] = "false"
    
    # TODO: ë¡œì»¬ ì²´í¬í¬ì¸íŠ¸ ê²½ë¡œ ì²˜ë¦¬ (glob íŒ¨í„´ ì§€ì›)
    import glob
    
    if "*" in pretrained:
        # Glob íŒ¨í„´ìœ¼ë¡œ ì²´í¬í¬ì¸íŠ¸ ì°¾ê¸°
        checkpoint_paths = glob.glob(pretrained)
        if checkpoint_paths:
            # TODO: ê°€ì¥ ìµœê·¼ íŒŒì¼ ì„ íƒ
            pretrained = # TODO: ì™„ì„±í•˜ì„¸ìš” (íŒŒì¼ ìˆ˜ì • ì‹œê°„ ê¸°ì¤€)
        else:
            raise ValueError(f"No checkpoint found matching pattern: {pretrained}")

    print(f"Starting Generative QA service with model: {pretrained}")
    
    # TODO: ìƒì„±í˜• QA ëª¨ë¸ ë¡œë“œ
    model = # TODO: GenerativeQAModel ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
    
    # TODO: Flask ì•± ìƒì„±
    app = # TODO: Flask(__name__, template_folder=Path("templates").resolve())

    # TODO: ì›¹ ì„œë¹„ìŠ¤ ì‹¤í–‰
    # TODO: model.run_server() í˜¸ì¶œ


@main.command()
def test():
    """
    ìƒì„±í˜• QA ëª¨ë¸ í…ŒìŠ¤íŠ¸ (í‘œì¤€ vs ì°½ì˜ì  ìƒì„± ë¹„êµ)
    """
    # TODO: í…ŒìŠ¤íŠ¸ìš© ëª¨ë¸ ë¡œë“œ
    pretrained = # TODO: ê¸°ë³¸ ëª¨ë¸ ê²½ë¡œ ì„¤ì •
    model = GenerativeQAModel(pretrained=pretrained, server_page="", num_beams=5, max_length=50)
    
    # TODO: í…ŒìŠ¤íŠ¸ ë°ì´í„°
    test_cases = [
        {
            "question": "ëŒ€í•œë¯¼êµ­ì˜ ìˆ˜ë„ëŠ”?",
            "context": "ëŒ€í•œë¯¼êµ­ì€ ë™ì•„ì‹œì•„ì— ìœ„ì¹˜í•œ ë‚˜ë¼ì´ë‹¤. ìˆ˜ë„ëŠ” ì„œìš¸íŠ¹ë³„ì‹œì´ë‹¤."
        },
        {
            "question": "ëŒ€í•œë¯¼êµ­ì— ëŒ€í•´ ì„¤ëª…í•´ì£¼ì„¸ìš”",  # ì°½ì˜ì  ì§ˆë¬¸!
            "context": "ëŒ€í•œë¯¼êµ­ì€ ë™ì•„ì‹œì•„ì˜ í•œë°˜ë„ ë‚¨ë¶€ì— ìœ„ì¹˜í•œ ë‚˜ë¼ì´ë‹¤. ìˆ˜ë„ëŠ” ì„œìš¸íŠ¹ë³„ì‹œì´ë©°, ì¸êµ¬ëŠ” ì•½ 5ì²œë§Œëª…ì´ë‹¤."
        }
    ]
    
    print("=== ìƒì„±í˜• QA ëª¨ë¸ í…ŒìŠ¤íŠ¸ ===")
    for i, test_case in enumerate(test_cases, 1):
        print(f"\n--- í…ŒìŠ¤íŠ¸ {i} ---")
        
        # TODO: í‘œì¤€ ìƒì„±
        standard_result = # TODO: model.infer_one() í˜¸ì¶œ
        
        # TODO: ì°½ì˜ì  ìƒì„±
        creative_result = # TODO: model.infer_creative() í˜¸ì¶œ
        
        print(f"Question: {standard_result['question']}")
        print(f"Standard Answer: {standard_result['answer']}")
        print(f"Standard Score: {standard_result['score']}")
        print(f"Creative Answer: {creative_result['answer']}")
        print(f"Creative Params: {creative_result['generation_params']}")


@main.command()
def compare_qa_types():
    """
    Extractive QA vs Generative QA ë¹„êµ ë°ëª¨
    """
    print("=== Extractive QA vs Generative QA ë¹„êµ ===")
    print()
    print("ğŸ“Š íŠ¹ì§• ë¹„êµ:")
    print("â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”")
    print("â”‚ íŠ¹ì§•            â”‚ Extractive QA       â”‚ Generative QA       â”‚")
    print("â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤")
    print("â”‚ ë‹µë³€ ë°©ì‹       â”‚ ì»¨í…ìŠ¤íŠ¸ì—ì„œ ì¶”ì¶œ   â”‚ ìƒˆë¡œìš´ í…ìŠ¤íŠ¸ ìƒì„±  â”‚")
    print("â”‚ ëª¨ë¸ êµ¬ì¡°       â”‚ BERT (Encoder)      â”‚ T5 (Encoder-Decoder)â”‚")
    print("â”‚ í›„ì²˜ë¦¬ ë³µì¡ë„   â”‚ ë§¤ìš° ë†’ìŒ â­â­â­â­â­   â”‚ ë§¤ìš° ë‚®ìŒ â­        â”‚")
    print("â”‚ ì°½ì˜ì  ë‹µë³€     â”‚ ë¶ˆê°€ëŠ¥              â”‚ ê°€ëŠ¥ âœ…             â”‚")
    print("â”‚ ì‚¬ì‹¤ì„± ë³´ì¥     â”‚ ë†’ìŒ âœ…             â”‚ ë‚®ìŒ (ê²€ì¦ í•„ìš”)   â”‚")
    print("â”‚ ì²˜ë¦¬ ì†ë„       â”‚ ë¹ ë¦„ âœ…             â”‚ ëŠë¦¼               â”‚")
    print("â”‚ ìš”ì•½/ì„¤ëª…       â”‚ ë¶ˆê°€ëŠ¥              â”‚ ê°€ëŠ¥ âœ…             â”‚")
    print("â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜")
    print()
    print("ğŸ¯ ì‚¬ìš© ì‹œë‚˜ë¦¬ì˜¤:")
    print("Extractive QA: ì •í™•í•œ ì •ë³´ ì¶”ì¶œ, íŒ©íŠ¸ ì²´í‚¹, ë¹ ë¥¸ ì‘ë‹µ")
    print("Generative QA: ì„¤ëª…, ìš”ì•½, ì°½ì˜ì  ë‹µë³€, êµìœ¡ ë„êµ¬")


if __name__ == "__main__":
    main()

"""
í•™ìŠµ ëª©í‘œ:
1. ìƒì„±í˜• QAì˜ ë…íŠ¹í•œ ì›¹ ì„œë¹„ìŠ¤ êµ¬í˜„ ë°©ë²• ì´í•´
2. Extractive QAì™€ì˜ ì„œë¹„ìŠ¤ ì„¤ê³„ ì°¨ì´ì  íŒŒì•…
3. ì°½ì˜ì  ìƒì„± íŒŒë¼ë¯¸í„°ì˜ ì‹¤ì œ í™œìš© ì²´í—˜
4. ë‘ ê°€ì§€ QA ë°©ì‹ì˜ ì¥ë‹¨ì  ë¹„êµ ë¶„ì„

í•µì‹¬ ê°œë…:

1. Generative QA ì„œë¹„ìŠ¤ íŠ¹ì§•:
   - í…ìŠ¤íŠ¸ ìƒì„± ê¸°ë°˜ ë‹µë³€
   - ì°½ì˜ì  ìƒì„± íŒŒë¼ë¯¸í„° ì§€ì›
   - ìš”ì•½, ì„¤ëª…, ì¶”ë¡  ë“± ë³µí•© ë‹µë³€ ê°€ëŠ¥
   - ì‚¬ìš©ì ë§ì¶¤í˜• ìƒì„± ì˜µì…˜

2. ì„œë¹„ìŠ¤ ì•„í‚¤í…ì²˜ ë¹„êµ:

   **Extractive QA ì„œë¹„ìŠ¤**:
   - ë‹¨ìˆœí•œ ì…ë ¥-ì¶œë ¥ êµ¬ì¡°
   - ê³ ì •ëœ ì¶”ë¡  ë°©ì‹
   - ë¹ ë¥¸ ì‘ë‹µ ì‹œê°„
   - ì œí•œëœ ë‹µë³€ í˜•íƒœ

   **Generative QA ì„œë¹„ìŠ¤**:
   - ìœ ì—°í•œ ìƒì„± íŒŒë¼ë¯¸í„°
   - ë‹¤ì–‘í•œ ìƒì„± ëª¨ë“œ (í‘œì¤€/ì°½ì˜ì )
   - ëŠë¦° ì‘ë‹µ ì‹œê°„
   - ë‹¤ì–‘í•œ ë‹µë³€ í˜•íƒœ

3. ì°½ì˜ì  ìƒì„± íŒŒë¼ë¯¸í„°:
   - do_sample=True: í™•ë¥ ì  ìƒì„± í™œì„±í™”
   - temperature: ì°½ì˜ì„± ì¡°ì ˆ (0.7-1.2)
   - top_p: Nucleus sampling (0.8-0.95)
   - num_beams: í’ˆì§ˆê³¼ ë‹¤ì–‘ì„± ê· í˜•

4. API ì„¤ê³„ ì°¨ì´ì :

   **Extractive QA API**:
   ```json
   POST /api
   {"question": "...", "context": "..."}
   â†’
   {"answer": "...", "score": 0.95, "start": 10, "end": 15}
   ```

   **Generative QA API**:
   ```json
   POST /api
   {"question": "...", "context": "..."}
   â†’
   {"answer": "...", "score": 0.85, "generation_params": {...}}
   
   POST /api/creative
   {"question": "...", "context": "...", "creative_params": {...}}
   â†’
   {"answer": "...", "generation_params": {...}}
   ```

5. ì‹¤ì‹œê°„ ìƒì„± ìµœì í™”:
   - Beam Search í¬ê¸° ì¡°ì ˆ
   - ìµœëŒ€ ê¸¸ì´ ì œí•œ
   - ë°°ì¹˜ ì²˜ë¦¬ í™œìš©
   - GPU ë©”ëª¨ë¦¬ ê´€ë¦¬

6. ì„œë¹„ìŠ¤ ëª¨ë‹ˆí„°ë§:
   - ìƒì„± ì‹œê°„ ì¸¡ì •
   - ë‹µë³€ í’ˆì§ˆ ì¶”ì 
   - ì‚¬ìš©ì ë§Œì¡±ë„ ìˆ˜ì§‘
   - ìƒì„± íŒŒë¼ë¯¸í„° ìµœì í™”

7. ì°½ì˜ì  í™œìš© ì‚¬ë¡€:
   - êµìœ¡: "ì„¤ëª…í•´ì£¼ì„¸ìš”", "ìš”ì•½í•´ì£¼ì„¸ìš”"
   - ê³ ê°ì§€ì›: "í•´ê²° ë°©ë²•ì„ ì•Œë ¤ì£¼ì„¸ìš”"
   - ì—°êµ¬: "ì´ê²ƒì˜ ì˜ë¯¸ëŠ” ë¬´ì—‡ì¸ê°€ìš”?"
   - ì°½ì‘: "ì´ì•¼ê¸°ë¥¼ ë§Œë“¤ì–´ì£¼ì„¸ìš”"

8. í’ˆì§ˆ ê´€ë¦¬:
   - ìƒì„± ê²°ê³¼ í•„í„°ë§
   - ë¶€ì ì ˆí•œ ë‚´ìš© íƒì§€
   - ì‚¬ì‹¤ì„± ê²€ì¦ ì‹œìŠ¤í…œ
   - ì‚¬ìš©ì í”¼ë“œë°± ìˆ˜ì§‘

ì‹¤ë¬´ ë°°í¬ ê³ ë ¤ì‚¬í•­:

1. ì„±ëŠ¥ ìµœì í™”:
   - ëª¨ë¸ ì–‘ìí™” ì ìš©
   - ìºì‹± ì „ëµ êµ¬í˜„
   - ë¹„ë™ê¸° ì²˜ë¦¬ ë„ì…
   - ë¡œë“œ ë°¸ëŸ°ì‹± ì„¤ì •

2. í’ˆì§ˆ ë³´ì¥:
   - ìƒì„± ê²°ê³¼ ê²€ì¦
   - ë…ì„± ì½˜í…ì¸  í•„í„°ë§
   - ì‚¬ì‹¤ì„± ì²´í¬ ì‹œìŠ¤í…œ
   - í¸í–¥ì„± ëª¨ë‹ˆí„°ë§

3. ì‚¬ìš©ì ê²½í—˜:
   - ì‹¤ì‹œê°„ ìƒì„± ì§„í–‰ë¥  í‘œì‹œ
   - ë‹¤ì–‘í•œ ìƒì„± ì˜µì…˜ ì œê³µ
   - ê²°ê³¼ ë§Œì¡±ë„ í”¼ë“œë°±
   - ê°œì¸í™” ì„¤ì • ì§€ì›

4. í™•ì¥ì„±:
   - ë‹¤ì¤‘ ëª¨ë¸ ì§€ì›
   - A/B í…ŒìŠ¤íŠ¸ í”„ë ˆì„ì›Œí¬
   - ì‹¤ì‹œê°„ ëª¨ë¸ ì—…ë°ì´íŠ¸
   - ê¸€ë¡œë²Œ ì„œë¹„ìŠ¤ ëŒ€ì‘

í™œìš© ë¶„ì•¼:
- êµìœ¡ í”Œë«í¼: ì„¤ëª…í˜• QA ì‹œìŠ¤í…œ
- ê³ ê° ì§€ì›: ë³µí•© ë¬¸ì œ í•´ê²° ê°€ì´ë“œ
- ì½˜í…ì¸  í”Œë«í¼: ì°½ì˜ì  ë‹µë³€ ìƒì„±
- ì—°êµ¬ ë„êµ¬: ë…¼ë¬¸ ìš”ì•½ ë° í•´ì„
- ì–¸ì–´ í•™ìŠµ: ëŒ€í™”í˜• íŠœí„° ì‹œìŠ¤í…œ
"""
